{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec8b89e4-27f2-4b9f-b569-0f7d288a367f",
   "metadata": {},
   "source": [
    "# Notebook for performing Mann-Whitney U-tests for statistical significance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f339d-0377-411b-a605-81389a7f7bdc",
   "metadata": {},
   "source": [
    "### Import stats libraries, namely mannwhitney package from scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cf8a309-2623-40ee-996d-6a4cc2dfe586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "from scipy.stats import mannwhitneyu\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafe5d12-cd51-4c33-bb5d-52656f40b586",
   "metadata": {},
   "source": [
    "## Import WRF daily max files (resampled to convective daily max) for each climate epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d990acde-634b-4b30-8c1f-6fbbad89c8da",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "did not find a match in any of xarray's currently installed IO backends ['netcdf4', 'h5netcdf', 'scipy', 'cfgrib', 'pydap', 'pynio', 'zarr']. Consider explicitly selecting one of the installed engines via the ``engine`` parameter, or installing additional IO dependencies, see:\nhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html\nhttps://docs.xarray.dev/en/stable/user-guide/io.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#wrf_hist = xr.open_dataset('/home/scratch/jgoodin/convective_daily_max_resamples/hist_daily_max_hail_inches.nc')\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wrf_mid4p5 \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/scratch/jgoodin/convective_daily_max_resamples/mid_century_4p5_daily_max_hail_inches.nc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m wrf_mid8p5 \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/scratch/jgoodin/convective_daily_max_resamples/mid_century_8p5_daily_max_hail_inches.nc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m wrf_end4p5 \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/scratch/jgoodin/convective_daily_max_resamples/end_century_4p5_daily_max_hail_inches.nc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/anaconda3/envs/pyEAE/lib/python3.9/site-packages/xarray/backends/api.py:479\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, backend_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(backend_kwargs)\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 479\u001b[0m     engine \u001b[38;5;241m=\u001b[39m \u001b[43mplugins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguess_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m backend \u001b[38;5;241m=\u001b[39m plugins\u001b[38;5;241m.\u001b[39mget_backend(engine)\n\u001b[1;32m    483\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    484\u001b[0m     decode_cf,\n\u001b[1;32m    485\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    491\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    492\u001b[0m )\n",
      "File \u001b[0;32m/anaconda3/envs/pyEAE/lib/python3.9/site-packages/xarray/backends/plugins.py:148\u001b[0m, in \u001b[0;36mguess_engine\u001b[0;34m(store_spec)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound the following matches with the input file in xarray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms IO \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackends: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompatible_engines\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. But their dependencies may not be installed, see:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.xarray.dev/en/stable/user-guide/io.html \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m     )\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n",
      "\u001b[0;31mValueError\u001b[0m: did not find a match in any of xarray's currently installed IO backends ['netcdf4', 'h5netcdf', 'scipy', 'cfgrib', 'pydap', 'pynio', 'zarr']. Consider explicitly selecting one of the installed engines via the ``engine`` parameter, or installing additional IO dependencies, see:\nhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html\nhttps://docs.xarray.dev/en/stable/user-guide/io.html"
     ]
    }
   ],
   "source": [
    "wrf_hist = xr.open_dataset('/home/scratch/jgoodin/convective_daily_max_resamples/hist_daily_max_hail_inches.nc')\n",
    "wrf_mid4p5 = xr.open_dataset('/home/scratch/jgoodin/convective_daily_max_resamples/mid_century_4p5_daily_max_hail_inches.nc')\n",
    "wrf_mid8p5 = xr.open_dataset('/home/scratch/jgoodin/convective_daily_max_resamples/mid_century_8p5_daily_max_hail_inches.nc')\n",
    "wrf_end4p5 = xr.open_dataset('/home/scratch/jgoodin/convective_daily_max_resamples/end_century_4p5_daily_max_hail_inches.nc')\n",
    "wrf_end8p5 = xr.open_dataset('/home/scratch/jgoodin/convective_daily_max_resamples/end_century_8p5_daily_max_hail_inches.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e435b0-e6f7-411f-8169-90a74967224d",
   "metadata": {},
   "source": [
    "## Pass hail day threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ff3d6f-8ce2-44e0-9e6c-97ec8f247345",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_sev = 1 * (wrf_hist >= 0.0254)\n",
    "mid4p5_sev = 1 * (wrf_mid4p5 >= 0.0254)\n",
    "mid8p5_sev = 1 * (wrf_mid8p5 >= 0.0254) #Can change threshold to larger hail as well\n",
    "end4p5_sev = 1 * (wrf_end4p5 >= 0.0254)\n",
    "end8p5_sev = 1 * (wrf_end8p5 >= 0.0254)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a634411b-8ebe-4413-97b9-16c75af2832c",
   "metadata": {},
   "source": [
    "### Resample by year and sum along the 'Time' dimension- provides a count of annual severe hail days for each simulation year. Take mean along 'year' dimension to provide annual sev hail day count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a315f65-102e-427e-8bf6-ca226958cc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_annual = hist_sev.groupby('Time.year').sum(dim = 'Time').mean(dim = 'year')\n",
    "mid4p5_annual = mid4p5_sev.groupby('Time.year').sum(dim = 'Time').mean(dim = 'year')\n",
    "mid8p5_annual = mid8p5_sev.groupby('Time.year').sum(dim = 'Time').mean(dim = 'year') #Groupby year and sum sev days across time dim to provide annual sev hail day count\n",
    "end4p5_annual = end4p5_sev.groupby('Time.year').sum(dim = 'Time').mean(dim = 'year') #Can change to seasonal as well\n",
    "end8p5_annual = end8p5_sev.groupby('Time.year').sum(dim = 'Time').mean(dim = 'year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c0b62b-618a-4b2e-9653-6d1e0aa776c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_annual.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02edbabe-53ec-4f79-94fc-d7705bb025e5",
   "metadata": {},
   "source": [
    "### Select array containing HAIL_MAX2D values to send into Mann-Whitney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5700144a-cdf1-44d0-ac89-6df9ef805ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_annual = hist_annual.HAIL_MAX2D\n",
    "mid4p5_annual = mid4p5_annual.HAIL_MAX2D\n",
    "mid8p5_annual = mid8p5_annual.HAIL_MAX2D #Select values (2D array) of HAIL_MAX2D to pass to Mann-Whitney U-test\n",
    "end4p5_annual = end4p5_annual.HAIL_MAX2D\n",
    "end8p5_annual = end8p5_annual.HAIL_MAX2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734647cd-50b5-4a0e-9dc1-726900755dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_annual.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d4d976-2564-47d4-81c3-1e24ab961e0e",
   "metadata": {},
   "source": [
    "## Import netcdf file containing lat/lon coords for WRF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b300690c-b1e4-4d03-93df-18cbe6dc6e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = xr.open_dataset('lat_lon.nc') #Open .nc file containing WRF lat/lon coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2a6bf2-b0ef-44e6-b08b-d8f25d0195dc",
   "metadata": {},
   "source": [
    "### Assign lat/lon coords from file to variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946fe7b2-1c4b-4c3e-9b04-8181f2a4385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = coords.CLAT.values[0, :, :]\n",
    "lons = coords.CLONG.values[0, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f59aae-ea82-4b41-9022-ae4703de637a",
   "metadata": {},
   "source": [
    "## Perform Mann-Whitney U-test for medians (means?), incorporating false-discovery rate correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f16ff4-bb93-43e6-bd07-c2a9b2bf9b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def field_significance(hist_annual, mid4p5_annual):\n",
    "    \n",
    "    #sets up p-value array (default 1 - not significant)\n",
    "    results = np.ones(shape=(hist_annual.shape[1], hist_annual.shape[2]), dtype=float)\n",
    "    \n",
    "    #loop through the y and x dimensions\n",
    "    for i in range(hist_annual.shape[1]):\n",
    "        for j in range(hist_annual.shape[2]):\n",
    "            \n",
    "            dist1 = hist_annual[:, i, j]\n",
    "            dist2 = mid4p5_annual[:, i, j]\n",
    "            \n",
    "            try:\n",
    "                s, p = mannwhitneyu(dist1, dist2)\n",
    "                results[i, j] = p\n",
    "            except Exception as e:\n",
    "                results[i, j] = np.nan\n",
    "            \n",
    "    return results\n",
    "\n",
    "#Send your arrays into 'field_significance' function, sample below\n",
    "res = field_significance(hist_annual, mid4p5_annual)\n",
    "#clip to your region if needed\n",
    "#res_mask = np.ma.masked_where(clip==False, res).filled(np.nan)\n",
    "#flat_max = mask_fin[0].values.flatten()\n",
    "\n",
    "#Perform the false discovery rate test\n",
    "p = multipletests(res.flatten(), alpha=0.1, method='fdr_bh')[0]\n",
    "\n",
    "#Save output\n",
    "#np.save('/home/scratch/jgoodin/fdr_delta_hist_mid4p5_annual_sev_hail_days.npy',p.reshape((lons.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f811c1-dcd5-4890-b50f-f05448df6993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyEAE]",
   "language": "python",
   "name": "conda-env-pyEAE-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
